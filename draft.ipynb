{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f285a-0992-457d-9c4c-396057bfd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the mutation and plotting related functions are in the mutation.py file\n",
    "from mutation import * \n",
    "from custom_utils import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374bf75-3b6a-46d0-adb5-fd0aa74907f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/fazle/notebook-ws/UCR_TimeSeriesAnomalyDatasets2021/AnomalyDatasets_2021/UCR_TimeSeriesAnomalyDatasets2021/FilesAreInHere/UCR_Anomaly_FullData/'\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter only .txt files\n",
    "txt_files = [file for file in files if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b7f72-88b2-49c4-84b6-6d74e4c63eb2",
   "metadata": {},
   "source": [
    "**Multi Layer Perceptron-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dadef-c7a3-4b37-b77d-64eaa3afdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_records = load_data(txt_files, folder_path, is_record = True, fraction_of_anomaly = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521baa0-7fd0-4966-9da6-97f52cef0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_mlp1(df_tuple, threshold = 0.10):\n",
    "    # Extract features and labels\n",
    "    X = df_tuple[0]['feature'].values.reshape(-1, 1)\n",
    "    y = df_tuple[0]['is_anomaly'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[:last_training_data], X[last_training_data:], y[:last_training_data], y[last_training_data:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    class_weight = {0: 1.0, 1: 20.0}\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping], class_weight=class_weight)\n",
    "    # model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    correct_or_not = is_prediction_correct( (np.argmax(y_pred) + last_training_data), begin_anomaly, end_anomaly)\n",
    "    y_pred = (y_pred > threshold).astype(int)\n",
    "    specificity = custom_specificity(y_test, y_pred, tolerance = end_anomaly-begin_anomaly)\n",
    "    return correct_or_not, specificity\n",
    "\n",
    "# List to store the results\n",
    "accuracy_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "for i, df in enumerate(dataset_records):\n",
    "    print(f'Training and evaluating on dataset {i+1}')\n",
    "    accuracy, specificity = train_and_evaluate_mlp1(df, threshold = 0.15)\n",
    "    specificity_scores.append(specificity)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f'Dataset {i+1} specificity: {specificity:.4f} Correctly_predicted: {accuracy}')\n",
    "\n",
    "# Calculate the average precision, recall, and F1 score across all datasets\n",
    "average_specificity = np.mean(specificity_scores)\n",
    "average_accuracy =  sum(1 for item in accuracy_scores if item == True) / len(accuracy_scores)\n",
    "print(f'Average specificity: {average_specificity:.4f}')\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d16f4a-720f-443a-a429-9887ecc1dfa0",
   "metadata": {},
   "source": [
    "**Multi Layer Perceptron-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cd9fa-5775-47d7-b745-87f95a5d0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seq = load_data(txt_files, folder_path, is_record = False, fraction_of_anomaly = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ea052-a1bb-44e6-ac3c-b7b40e8637f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, labels, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(labels[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_and_evaluate_mlp2(df_tuple, window_size=20, threshold=0.10):\n",
    "    # Extract features and labels\n",
    "    X = df_tuple[0]['feature'].values\n",
    "    y = df_tuple[0]['is_anomaly'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences = create_sequences(X, y, window_size)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test = X_sequences[:last_training_data - window_size], X_sequences[last_training_data - window_size:]\n",
    "    y_train, y_test = y_sequences[:last_training_data - window_size], y_sequences[last_training_data - window_size:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(window_size,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    class_weight = {0: 1.0, 1: 20.0}\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    # model.fit(X_train, y_train, epochs=70, batch_size=32, validation_split=0.2, class_weight=class_weight)\n",
    "\n",
    "\n",
    "    # model.compile(optimizer='adam', loss=custom_loss)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping], class_weight=class_weight)\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    correct_or_not = is_prediction_correct((np.argmax(y_pred) + last_training_data), begin_anomaly, end_anomaly)\n",
    "    y_pred = (y_pred > threshold).astype(int)\n",
    "    specificity = custom_specificity(y_test, y_pred, tolerance=end_anomaly - begin_anomaly)\n",
    "    return correct_or_not, specificity\n",
    "\n",
    "# # Usage\n",
    "# df_tuple = (df, last_training_data, begin_anomaly, end_anomaly)\n",
    "# correct_or_not, specificity = train_and_evaluate_mlp(df_tuple)\n",
    "# List to store the results\n",
    "accuracy_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "for i, df in enumerate(dataset_seq):\n",
    "    print(f'Training and evaluating on dataset {i+1}')\n",
    "    accuracy, specificity = train_and_evaluate_mlp2(df, threshold = 0.15)\n",
    "    specificity_scores.append(specificity)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f'Dataset {i+1} specificity: {specificity:.4f} Correctly_predicted: {accuracy}')\n",
    "\n",
    "# Calculate the average precision, recall, and F1 score across all datasets\n",
    "average_specificity = np.mean(specificity_scores)\n",
    "average_accuracy =  sum(1 for item in accuracy_scores if item == True) / len(accuracy_scores)\n",
    "print(f'Average specificity: {average_specificity:.4f}')\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc9239-9465-402d-803a-28857ba16f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9242fd-e438-476c-a454-6a2633c49e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb4522-9039-49b0-9876-dec062515bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3accd-7793-44ba-9dd3-7342471a41a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e8a06e2-6bba-450c-be84-27089277360d",
   "metadata": {},
   "source": [
    "# Anomalous sequence detection - semi supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa364d-5582-433a-abce-1c4602b4a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 50\n",
    "segment_length = 50\n",
    "\n",
    "dataset_seq_wo_mutation = load_data(txt_files, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a1a1c-ef0b-45cf-8502-cd85cae85ae3",
   "metadata": {},
   "source": [
    "**LSTM Auto-encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b311ae2-4e9c-4e7a-bfa5-d737a978624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_autoencoder(timesteps, input_dim):\n",
    "    inputs = Input(shape=(timesteps, input_dim))\n",
    "    # Encoder\n",
    "    encoded = LSTM(64, activation='relu')(inputs)\n",
    "    encoded = RepeatVector(timesteps)(encoded)\n",
    "    # Decoder\n",
    "    decoded = LSTM(64, activation='relu', return_sequences=True)(encoded)\n",
    "    decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "def fit_lstm_autoencoder(model, train_data, epochs=80, batch_size=32):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(train_data, train_data, epochs=epochs, verbose=0, batch_size=batch_size, \n",
    "              validation_split=0.2, callbacks=[early_stopping], shuffle=False)\n",
    "    return model\n",
    "\n",
    "def detect_anomaly(model, test_data, segment_length):\n",
    "    predictions = model.predict(test_data)\n",
    "    reconstruction_errors = np.mean(np.abs(predictions - test_data), axis=1)\n",
    "    segment_errors = []\n",
    "\n",
    "    # Calculate average error for each segment\n",
    "    for i in range(len(reconstruction_errors) - segment_length + 1):\n",
    "        segment_error = np.mean(reconstruction_errors[i:i + segment_length])\n",
    "        segment_errors.append(segment_error)\n",
    "\n",
    "    # Find the segment with the highest average error\n",
    "    most_confident_anomalous_segment = np.argmax(segment_errors)\n",
    "    center_of_anomalous_segment = most_confident_anomalous_segment + segment_length // 2\n",
    "\n",
    "    return center_of_anomalous_segment\n",
    "\n",
    "def train_and_evaluate_lstm_autoencoder(train_df, test_df_list, timesteps, segment_length):\n",
    "    X = train_df['feature'].values\n",
    "\n",
    "\n",
    "    # Create sequences\n",
    "    train_data = create_sequences(X, timesteps)\n",
    "    train_data = train_data.reshape((train_data.shape[0], timesteps, 1))\n",
    "\n",
    "    # Create and train the LSTM-Autoencoder\n",
    "    model = create_lstm_autoencoder(timesteps, 1)\n",
    "    model = fit_lstm_autoencoder(model, train_data)\n",
    "\n",
    "    answers = []\n",
    "    for test_d in test_df_list:\n",
    "        test_data = test_d[0]\n",
    "        b_anomaly = test_d[1]\n",
    "        e_anomaly = test_d[2]\n",
    "        test_data = create_sequences(test_data, timesteps)\n",
    "        test_data = test_data.reshape((test_data.shape[0], timesteps, 1))\n",
    "        center_of_anomalous_segment = detect_anomaly(model, test_data, segment_length)\n",
    "        correct = is_prediction_correct_seq(center_of_anomalous_segment, b_anomaly, e_anomaly)\n",
    "        answers.append(correct)\n",
    "\n",
    "    acc = sum(1 for item in answers if item == True) / len(answers)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34251426-c166-451d-b0fd-1e325e6d4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, df in enumerate(dataset_seq_wo_mutation):\n",
    "    train_df = df[0]\n",
    "    test_df_list = df[1]\n",
    "    print(f'Training and evaluating on dataset {i + 1}')\n",
    "    acc = train_and_evaluate_lstm_autoencoder(train_df, test_df_list, timesteps, segment_length)\n",
    "    results.append(acc)\n",
    "    print(f'Dataset {i + 1} Accuracy: {acc}')\n",
    "\n",
    "accuracy = sum(results) / len(results)\n",
    "print(f'Final Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbdf57-d3e1-47d3-8662-a210504fdff8",
   "metadata": {},
   "source": [
    "**Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a9dae-3db8-470a-94c7-b2d5bcdc69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def create_transformer_autoencoder(timesteps, input_dim, embed_dim, num_heads, ff_dim):\n",
    "    inputs = Input(shape=(timesteps, input_dim))\n",
    "    \n",
    "    # Encoder\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = Dense(embed_dim)(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Dense(embed_dim, activation=\"relu\")(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(embed_dim, activation=\"relu\")(x)\n",
    "    x = transformer_block(x)\n",
    "    decoded = Dense(input_dim)(x)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-4), loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "def fit_transformer_autoencoder(model, train_data, epochs=80, batch_size=32):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(train_data, train_data, epochs=epochs, verbose=0, batch_size=batch_size, \n",
    "              validation_split=0.2, callbacks=[early_stopping], shuffle=False)\n",
    "    return model\n",
    "\n",
    "def create_sequences(data, timesteps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - timesteps + 1):\n",
    "        sequence = data[i:i + timesteps]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def detect_anomaly(model, test_data, segment_length):\n",
    "    predictions = model.predict(test_data)\n",
    "    reconstruction_errors = np.mean(np.abs(predictions - test_data), axis=1)\n",
    "    segment_errors = []\n",
    "\n",
    "    for i in range(len(reconstruction_errors) - segment_length + 1):\n",
    "        segment_error = np.mean(reconstruction_errors[i:i + segment_length])\n",
    "        segment_errors.append(segment_error)\n",
    "\n",
    "    most_confident_anomalous_segment = np.argmax(segment_errors)\n",
    "    center_of_anomalous_segment = most_confident_anomalous_segment + segment_length // 2\n",
    "\n",
    "    return center_of_anomalous_segment\n",
    "\n",
    "def is_prediction_correct_seq(prediction, begin, end):\n",
    "    print(f'prediction: {prediction}, begin: {begin}, end: {end}')\n",
    "    L = end - begin + 1\n",
    "    return min(begin - L, begin - 100) < prediction < max(end + L, end + 100)\n",
    "\n",
    "def train_and_evaluate_transformer_autoencoder(df_tuple, timesteps, segment_length):\n",
    "    X = df_tuple[0]['feature'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "    \n",
    "    train_data, test_data = X[:last_training_data], X[last_training_data:]\n",
    "\n",
    "    train_data = create_sequences(train_data, timesteps)\n",
    "    test_data = create_sequences(test_data, timesteps)\n",
    "    \n",
    "    train_data = train_data.reshape((train_data.shape[0], timesteps, 1))\n",
    "    test_data = test_data.reshape((test_data.shape[0], timesteps, 1))\n",
    "    \n",
    "    model = create_transformer_autoencoder(timesteps, 1, embed_dim, num_heads, ff_dim)\n",
    "    model = fit_transformer_autoencoder(model, train_data)\n",
    "    \n",
    "    center_of_anomalous_segment = detect_anomaly(model, test_data, segment_length) + last_training_data\n",
    "    correct = is_prediction_correct_seq(center_of_anomalous_segment, begin_anomaly, end_anomaly)\n",
    "    return correct\n",
    "\n",
    "# Example usage\n",
    "timesteps = 50\n",
    "segment_length = 20\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "\n",
    "dataset_seq_wo_mutation = load_data(txt_files, folder_path, is_record=False, fraction_of_anomaly=0.02, mutation_done=False)\n",
    "results = []\n",
    "\n",
    "for i, df in enumerate(dataset_seq_wo_mutation):\n",
    "    print(f'Training and evaluating on dataset {i + 1}')\n",
    "    correct_or_not = train_and_evaluate_transformer_autoencoder(df, timesteps, segment_length)\n",
    "    results.append(correct_or_not)\n",
    "    print(f'Dataset {i + 1} correctly identified: {correct_or_not}')\n",
    "\n",
    "accuracy = sum(1 for item in results if item == True) / len(results)\n",
    "print(f'Final Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236700e-96ad-41c7-9e1e-73127fe265fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eba1d-7e9b-4ea6-8d95-49a031c82400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
