{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f285a-0992-457d-9c4c-396057bfd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the mutation and plotting related functions are in the mutation.py file\n",
    "from mutation import * \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374bf75-3b6a-46d0-adb5-fd0aa74907f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/fazle/notebook-ws/UCR_TimeSeriesAnomalyDatasets2021/AnomalyDatasets_2021/UCR_TimeSeriesAnomalyDatasets2021/FilesAreInHere/UCR_Anomaly_FullData/'\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter only .txt files\n",
    "txt_files = [file for file in files if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719f993-264d-49fb-8230-edd495b73fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss and metrics for my models\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     anomaly_weight = tf.constant(100.0, dtype=tf.float32)  # Set a higher weight for anomalies\n",
    "#     normal_weight = tf.constant(1.0, dtype=tf.float32)    # Weight for normal points\n",
    "#     base_loss = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "\n",
    "#     weight_vector = tf.where(\n",
    "#         tf.math.logical_and(y_true == 1, y_pred == 0),\n",
    "#         anomaly_weight,  # Apply anomaly weight if the true label is 1 and the prediction is 0\n",
    "#         normal_weight  # Apply normal weight otherwise\n",
    "#     )\n",
    "\n",
    "#     # Weighted loss calculation\n",
    "#     weighted_loss = base_loss * weight_vector\n",
    "\n",
    "#     # Return the mean of the weighted loss\n",
    "#     return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "\n",
    "def custom_specificity(y_true, y_pred, tolerance=100):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    total_predictions = len(y_pred)\n",
    "    \n",
    "    tolerance = max(tolerance, 100)\n",
    "\n",
    "    for i in range(total_predictions):\n",
    "        if y_pred[i] == 1:\n",
    "            # Check if there is any anomaly in the ground truth within the range p-100 to p+100\n",
    "            if np.any(y_true[max(0, i - tolerance):min(len(y_true), i + tolerance + 1)] == 1):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 0:\n",
    "            tn += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    specificity = tn / (tn + fp) if (tn+fp) > 0 else 0.0\n",
    "    return specificity\n",
    "    \n",
    "# Define the custom accuracy function\n",
    "def is_prediction_correct(prediction, begin, end):\n",
    "    print(f'prediction: {prediction}, begin: {begin}, end: {end}')\n",
    "    L = end - begin + 1\n",
    "    return min(begin - L, begin - 100) < prediction < max(end + L, end + 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b7f72-88b2-49c4-84b6-6d74e4c63eb2",
   "metadata": {},
   "source": [
    "**Multi Layer Perceptron-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dadef-c7a3-4b37-b77d-64eaa3afdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_records = load_data(txt_files, folder_path, is_record = True, fraction_of_anomaly = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521baa0-7fd0-4966-9da6-97f52cef0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_mlp1(df_tuple, threshold = 0.10):\n",
    "    # Extract features and labels\n",
    "    X = df_tuple[0]['feature'].values.reshape(-1, 1)\n",
    "    y = df_tuple[0]['is_anomaly'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[:last_training_data], X[last_training_data:], y[:last_training_data], y[last_training_data:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    class_weight = {0: 1.0, 1: 20.0}\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping], class_weight=class_weight)\n",
    "    # model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    correct_or_not = is_prediction_correct( (np.argmax(y_pred) + last_training_data), begin_anomaly, end_anomaly)\n",
    "    y_pred = (y_pred > threshold).astype(int)\n",
    "    specificity = custom_specificity(y_test, y_pred, tolerance = end_anomaly-begin_anomaly)\n",
    "    return correct_or_not, specificity\n",
    "\n",
    "# List to store the results\n",
    "accuracy_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "for i, df in enumerate(dataset_records):\n",
    "    print(f'Training and evaluating on dataset {i+1}')\n",
    "    accuracy, specificity = train_and_evaluate_mlp1(df, threshold = 0.15)\n",
    "    specificity_scores.append(specificity)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f'Dataset {i+1} specificity: {specificity:.4f} Correctly_predicted: {accuracy}')\n",
    "\n",
    "# Calculate the average precision, recall, and F1 score across all datasets\n",
    "average_specificity = np.mean(specificity_scores)\n",
    "average_accuracy =  sum(1 for item in accuracy_scores if item == True) / len(accuracy_scores)\n",
    "print(f'Average specificity: {average_specificity:.4f}')\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d16f4a-720f-443a-a429-9887ecc1dfa0",
   "metadata": {},
   "source": [
    "**Multi Layer Perceptron-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cd9fa-5775-47d7-b745-87f95a5d0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seq = load_data(txt_files, folder_path, is_record = False, fraction_of_anomaly = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ea052-a1bb-44e6-ac3c-b7b40e8637f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, labels, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(labels[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_and_evaluate_mlp2(df_tuple, window_size=20, threshold=0.10):\n",
    "    # Extract features and labels\n",
    "    X = df_tuple[0]['feature'].values\n",
    "    y = df_tuple[0]['is_anomaly'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences = create_sequences(X, y, window_size)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test = X_sequences[:last_training_data - window_size], X_sequences[last_training_data - window_size:]\n",
    "    y_train, y_test = y_sequences[:last_training_data - window_size], y_sequences[last_training_data - window_size:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(window_size,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    class_weight = {0: 1.0, 1: 20.0}\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    # model.fit(X_train, y_train, epochs=70, batch_size=32, validation_split=0.2, class_weight=class_weight)\n",
    "\n",
    "\n",
    "    # model.compile(optimizer='adam', loss=custom_loss)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model.fit(X_train, y_train, epochs=70, verbose=0, batch_size=32, validation_split=0.2, callbacks=[early_stopping], class_weight=class_weight)\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    correct_or_not = is_prediction_correct((np.argmax(y_pred) + last_training_data), begin_anomaly, end_anomaly)\n",
    "    y_pred = (y_pred > threshold).astype(int)\n",
    "    specificity = custom_specificity(y_test, y_pred, tolerance=end_anomaly - begin_anomaly)\n",
    "    return correct_or_not, specificity\n",
    "\n",
    "# # Usage\n",
    "# df_tuple = (df, last_training_data, begin_anomaly, end_anomaly)\n",
    "# correct_or_not, specificity = train_and_evaluate_mlp(df_tuple)\n",
    "# List to store the results\n",
    "accuracy_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "for i, df in enumerate(dataset_seq):\n",
    "    print(f'Training and evaluating on dataset {i+1}')\n",
    "    accuracy, specificity = train_and_evaluate_mlp2(df, threshold = 0.15)\n",
    "    specificity_scores.append(specificity)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f'Dataset {i+1} specificity: {specificity:.4f} Correctly_predicted: {accuracy}')\n",
    "\n",
    "# Calculate the average precision, recall, and F1 score across all datasets\n",
    "average_specificity = np.mean(specificity_scores)\n",
    "average_accuracy =  sum(1 for item in accuracy_scores if item == True) / len(accuracy_scores)\n",
    "print(f'Average specificity: {average_specificity:.4f}')\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc9239-9465-402d-803a-28857ba16f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9242fd-e438-476c-a454-6a2633c49e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb4522-9039-49b0-9876-dec062515bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3accd-7793-44ba-9dd3-7342471a41a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e8a06e2-6bba-450c-be84-27089277360d",
   "metadata": {},
   "source": [
    "# Anomalous sequence detection - semi supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51cc5-3317-4fbc-9e8f-577d3c14f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_autoencoder(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu', return_sequences=False),\n",
    "        RepeatVector(input_shape[0]),\n",
    "        LSTM(32, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(input_shape[1]))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def fit_lstm_autoencoder(model, train_data, epochs=80, batch_size=32):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(train_data, train_data, epochs=epochs, verbose=0, batch_size=batch_size, \n",
    "              validation_split=0.2, callbacks=[early_stopping], shuffle=False)\n",
    "    return model\n",
    "\n",
    "def create_sequences(data, timesteps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - timesteps + 1):\n",
    "        sequence = data[i:i + timesteps]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def is_prediction_correct_seq(prediction, begin, end):\n",
    "    print(f'prediction: {prediction}, begin: {begin}, end: {end}')\n",
    "    L = end - begin + 1\n",
    "    return min(begin - L, begin - 100) < prediction < max(end + L, end + 100)\n",
    "\n",
    "def detect_anomaly(model, test_data, segment_length):\n",
    "    predictions = model.predict(test_data)\n",
    "    reconstruction_errors = np.mean(np.abs(predictions - test_data), axis=1)\n",
    "    segment_errors = []\n",
    "\n",
    "    # Calculate average error for each segment\n",
    "    for i in range(len(reconstruction_errors) - segment_length + 1):\n",
    "        segment_error = np.mean(reconstruction_errors[i:i + segment_length])\n",
    "        segment_errors.append(segment_error)\n",
    "\n",
    "    # Find the segment with the highest average error\n",
    "    most_confident_anomalous_segment = np.argmax(segment_errors)\n",
    "    center_of_anomalous_segment = most_confident_anomalous_segment + segment_length // 2\n",
    "\n",
    "    return center_of_anomalous_segment\n",
    "\n",
    "def train_and_evaluate_lstm_autoencoder(df_tuple, timesteps, segment_length):\n",
    "    X = df_tuple[0]['feature'].values\n",
    "    last_training_data = df_tuple[1]\n",
    "    begin_anomaly = df_tuple[2]\n",
    "    end_anomaly = df_tuple[3]\n",
    "    \n",
    "    train_data, test_data = X[:last_training_data], X[last_training_data:]\n",
    "\n",
    "    # Create sequences\n",
    "    train_data = create_sequences(train_data, timesteps)\n",
    "    test_data = create_sequences(test_data, timesteps)\n",
    "    \n",
    "    # Reshape data for LSTM\n",
    "    train_data = train_data.reshape((train_data.shape[0], timesteps, 1))\n",
    "    test_data = test_data.reshape((test_data.shape[0], timesteps, 1))\n",
    "    \n",
    "    # Create and train the LSTM-Autoencoder\n",
    "    model = create_lstm_autoencoder((timesteps, 1))\n",
    "    model = fit_lstm_autoencoder(model, train_data)\n",
    "    \n",
    "    # Detect anomalies in the test set\n",
    "    center_of_anomalous_segment = detect_anomaly(model, test_data, segment_length) + last_training_data\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    correct = is_prediction_correct_seq(center_of_anomalous_segment, begin_anomaly, end_anomaly)\n",
    "    # print(f'last training point: {last_training_data}')\n",
    "    return correct\n",
    "\n",
    "# Example number of timesteps and segment length\n",
    "timesteps = 32\n",
    "segment_length = 32\n",
    "\n",
    "dataset_seq_wo_mutation = load_data(txt_files, folder_path, is_record=False, fraction_of_anomaly=0.02, mutation_done=False)\n",
    "results = []\n",
    "\n",
    "for i, df in enumerate(dataset_seq_wo_mutation):\n",
    "    print(f'Training and evaluating on dataset {i + 1}')\n",
    "    correct_or_not = train_and_evaluate_lstm_autoencoder(df, timesteps, segment_length)\n",
    "    results.append(correct_or_not)\n",
    "    print(f'Dataset {i + 1} correctly identified: {correct_or_not}')\n",
    "\n",
    "accuracy = sum(1 for item in results if item == True) / len(results)\n",
    "print(f'Final Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c2143-f1a8-4bdb-a783-18f4075c5215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a9dae-3db8-470a-94c7-b2d5bcdc69fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437eb28-53bc-4890-8b8c-84f00b39785e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236700e-96ad-41c7-9e1e-73127fe265fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eba1d-7e9b-4ea6-8d95-49a031c82400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
